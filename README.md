# Description

I'm doing some research supported by [Cat's Grant](https://publish.obsidian.md/c1sc0/Projects/Cat%27s+Grant). I'll use this repo to publish progress.

## Research Questions

1. Do Large Language Models Understand Logic or Just Mimic Context? (Replication) 
2. How does implementation architecture impact correctness & speed of logical inference (Benchmarking)? 

## Relevant Publications

Yan, Junbing, Chengyu Wang, Jun Huang, and Wei Zhang. “Do Large Language Models Understand Logic or Just Mimick Context?” arXiv, February 19, 2024. [http://arxiv.org/abs/2402.12091](http://arxiv.org/abs/2402.12091).

Liu, Hanmeng, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, and Yue Zhang. “LogiCoT: Logical Chain-of-Thought Instruction-Tuning.” arXiv, October 28, 2023. [http://arxiv.org/abs/2305.12147](http://arxiv.org/abs/2305.12147).

Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv, January 10, 2023. [http://arxiv.org/abs/2201.11903](http://arxiv.org/abs/2201.11903).

Chollet, François. “On the Measure of Intelligence.” arXiv, November 25, 2019. [http://arxiv.org/abs/1911.01547](http://arxiv.org/abs/1911.01547).